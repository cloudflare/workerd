#!/usr/bin/python3
"""
Usage: update-deps.py [dep_name]
"""

import datetime
import hashlib
import io
import json
import os
import re
import subprocess
import sys
import tarfile
import urllib.request
import zipfile
from pathlib import Path

TARGET_FILTER = None if len(sys.argv) < 2 else sys.argv[1]

SCRIPT_DIR = Path(__file__).parent
if "BUILD_WORKSPACE_DIRECTORY" in os.environ:
    SCRIPT_DIR = Path(os.environ["BUILD_WORKSPACE_DIRECTORY"]) / "build" / "deps"

GEN_DIR = SCRIPT_DIR / "gen"
ALL_DEPS = ["deps.jsonc", "build_deps.jsonc", "shared_deps.jsonc"]


TOP = "# WARNING: THIS FILE IS AUTOGENERATED BY update-deps.py DO NOT EDIT\n"

GITHUB_TAR_URL_TEMPLATE = "https://github.com/{owner}/{repo}/tarball/{commit}"

GITHUB_RELEASE_FILE_URL_TEMPLATE = (
    "https://github.com/{owner}/{repo}/releases/download/v{version}/{file}"
)

DEP_TEMPLATE = (
    TOP
    + """
{rule_name} = use_repo_rule("{rule_file}", "{rule_name}")

{rule_name}({attrs}
)
"""
)


GITHUB_ACCESS_TOKEN = ""


def format_attr_list(attr_list):
    if not attr_list:
        return ""

    return "\n" + "\n".join(
        f"    {k} = {format_attr(v)}," for k, v in attr_list.items()
    )


def format_attr(v):
    if isinstance(v, (bool, int)):
        return str(v)
    else:
        return json.dumps(v)


def format_dep(repo, rule_file, rule_name, attrs):
    return DEP_TEMPLATE.format(
        rule_file=rule_file,
        rule_name=rule_name,
        macro_name=macro_name(repo),
        attrs=format_attr_list(repo_attributes(repo) | attrs),
    )


def macro_name(repo):
    return "dep_" + repo["name"].replace("-", "_")


class RateLimitedException(Exception):
    pass


class AssetsException(Exception):
    pass


class UnsupportedException(Exception):
    pass


def github_urlopen(url):
    """
    A wrapper around urllib.request.urlopen() which parses GitHub rate limit errors and
    provides a more human-friendly explanation.
    """
    if GITHUB_ACCESS_TOKEN != "":
        url = urllib.request.Request(url)
        url.add_header("Authorization", f"Bearer {GITHUB_ACCESS_TOKEN}")
    try:
        return urllib.request.urlopen(url)
    except urllib.error.HTTPError as e:
        reset_ts = e.headers["x-ratelimit-reset"]
        if e.code != 403 or not reset_ts:
            raise
        reset_dt = datetime.datetime.fromtimestamp(int(reset_ts))
        reset_iso_utc = reset_dt.astimezone(datetime.UTC).isoformat(" ")
        reset_iso_local = reset_dt.isoformat(" ")
        raise RateLimitedException(
            f"""
We have been rate-limited by GitHub. We can make API calls again at:
  {reset_iso_utc} UTC ({reset_iso_local} local time).
"""
            + """
You can try re-running the script and specifying an access token since authenticated
GitHub API requests have a higher rate limit.
"""
            if GITHUB_ACCESS_TOKEN == ""
            else ""
        ) from e


def github_last_commit(repo):
    owner = repo["owner"]
    github_repo = repo["repo"]
    branch = repo.get("branch", "master")
    api_url = f"https://api.github.com/repos/{owner}/{github_repo}/commits/{branch}"
    commits = json.loads(github_urlopen(api_url).read())
    return commits["sha"]


def get_url_content_sha256(url):
    return hashlib.sha256(urllib.request.urlopen(url).read()).hexdigest()


def repo_attributes(repo):
    repo_attrs = {}

    for option in (
        "name",
        "build_file",
        "repo_mapping",
        "downloaded_file_path",
        "build_file_content",
        "patches",
    ):
        if option in repo:
            repo_attrs[option] = repo[option]

    if "patches" in repo_attrs:
        repo_attrs["patch_args"] = ["-p1"]

    return repo_attrs


def gen_github_tarball(repo):
    owner = repo["owner"]
    github_repo = repo["repo"]

    commit = github_last_commit(repo)
    if "freeze_commit" in repo:
        if repo["freeze_commit"] != commit:
            print(
                "frozen, update available ",
                repo["freeze_commit"][:7],
                " -> ",
                commit[:7],
                end="",
            )
        commit = repo["freeze_commit"]
    else:
        print(commit[:7], end="")

    prefix = f"{owner}-{github_repo}-{commit[:7]}"
    if "extra_strip_prefix" in repo:
        prefix = prefix + repo["extra_strip_prefix"]

    url = GITHUB_TAR_URL_TEMPLATE.format(
        owner=owner,
        repo=github_repo,
        commit=commit,
    )

    if "freeze_sha256" in repo:
        sha256 = repo["freeze_sha256"]
    else:
        sha256 = get_url_content_sha256(url)

    return format_dep(
        repo,
        rule_file="@//:build/http.bzl",
        rule_name="http_archive",
        attrs=dict(
            url=url,
            strip_prefix=prefix,
            sha256=sha256,
            type="tgz",
        ),
    )


def github_last_release(repo):
    owner = repo["owner"]
    github_repo = repo["repo"]
    api_url = f"https://api.github.com/repos/{owner}/{github_repo}/releases/latest"
    return json.loads(github_urlopen(api_url).read())


def github_release(repo, tag_name):
    owner = repo["owner"]
    github_repo = repo["repo"]
    api_url = (
        f"https://api.github.com/repos/{owner}/{github_repo}/releases/tags/{tag_name}"
    )
    return json.loads(github_urlopen(api_url).read())


def gen_github_release(repo):
    try:
        release = github_last_release(repo)
    except urllib.error.HTTPError as e:
        # If a repo only has pre-releases, github_last_release will throw a 404 error.
        # In that case, we must specify a "freeze_version".
        if e.code != 404 or "freeze_version" not in repo:
            raise
        release = None

    if "freeze_version" in repo:
        frozen_release = github_release(repo, repo["freeze_version"])
        if release is not None and frozen_release["tag_name"] != release["tag_name"]:
            print(
                "frozen, update available: {} -> {}".format(
                    frozen_release["tag_name"], release["tag_name"]
                ),
                end="",
            )
        release = frozen_release
    else:
        print(release["tag_name"], end="")

    if "file_regex" in repo:
        # Using file_regex to select a user-uploaded asset
        url = get_release_asset(repo, release)
    else:
        # Using Github-generated tarball
        url = release["tarball_url"]

    type = "tgz"
    if url.endswith(".zip"):
        type = "zip"
    elif url.endswith(".xz"):
        type = "xz"
    elif url.endswith(".tar.bz2"):
        type = "tar.bz2"

    content = urllib.request.urlopen(url).read()

    if "freeze_sha256" in repo:
        sha256 = repo["freeze_sha256"]
    else:
        sha256 = hashlib.sha256(content).hexdigest()

    file_type = repo.get("file_type", "archive")
    if file_type == "archive":
        if "strip_prefix" in repo:
            prefix = repo["strip_prefix"]
        elif url.endswith(".zip"):
            with zipfile.ZipFile(io.BytesIO(content)) as zip:
                prefix = os.path.commonprefix(zip.namelist())
        else:
            with tarfile.open(fileobj=io.BytesIO(content)) as tgz:
                prefix = os.path.commonprefix(tgz.getnames())

        return format_dep(
            repo,
            rule_file="@//:build/http.bzl",
            rule_name="http_archive",
            attrs=dict(
                url=url,
                strip_prefix=prefix,
                sha256=sha256,
                type=type,
            ),
        )
    elif file_type == "executable":
        return format_dep(
            repo,
            rule_file="@//:build/http.bzl",
            rule_name="http_file",
            attrs=dict(url=url, sha256=sha256, executable=True),
        )
    else:
        raise UnsupportedException("Unsupported file_type: " + file_type)


def get_release_asset(repo, release):
    file_regex = re.compile(repo["file_regex"])
    assets = [a for a in release["assets"] if file_regex.match(a["name"])]

    if len(assets) == 0:
        raise AssetsException("No assets found: " + json.dumps(release))

    if len(assets) > 1:
        raise AssetsException(
            "Too many assets, use more specific file_regex: "
            + str([a["name"] for a in assets])
        )

    return assets[0]["browser_download_url"]


def gen_git_clone(repo):
    url = repo["url"]

    # We used to clone the repository here to get a shallow_since timestamp, but based
    # on # https://github.com/bazelbuild/bazel/issues/12857 it is unclear if this is
    # actually helpful.
    ls_remote = subprocess.run(
        ["git", "ls-remote", url, repo["branch"]], capture_output=True, text=True
    )
    ls_remote.check_returncode()
    commit = ls_remote.stdout.strip().split()[0]

    if "freeze_commit" in repo:
        freeze_commit = repo["freeze_commit"]
        if freeze_commit != commit:
            print(
                "frozen, update available ",
                repo["freeze_commit"][:7],
                " -> ",
                commit[:7],
                end="",
            )
            commit = freeze_commit
        else:
            print(commit[:7], end="")

    return format_dep(
        repo,
        rule_file="@bazel_tools//tools/build_defs/repo:git.bzl",
        rule_name="git_repository",
        attrs=dict(
            remote=url,
            commit=commit,
        ),
    )


def gen_repo_str(repo):
    if repo["type"] == "github_tarball":
        return gen_github_tarball(repo)
    elif repo["type"] == "github_release":
        return gen_github_release(repo)
    elif repo["type"] == "git_clone":
        return gen_git_clone(repo)
    else:
        raise UnsupportedException(f"Unsupported repo type: {repo['type']}")


def gen_repo_bzl(repo):
    print("Checking", repo["name"], "... ", end="", flush=True)
    if TARGET_FILTER is not None and not repo["name"].startswith(TARGET_FILTER):
        print("skipped")
        return
    with (GEN_DIR / f"{macro_name(repo)}.MODULE.bazel").open("w") as bzl_file:
        bzl_file.write(gen_repo_str(repo))
    print()


def gen_deps_bzl(deps, deps_bzl, is_shared=False):
    deps_bzl_content = TOP
    macro_names = [macro_name(repo) for repo in deps["repositories"]]

    for name in sorted(macro_names):
        # Buildifier prefers load statements to be sorted alphabetically.
        deps_bzl_content += f'\ninclude("//build/deps:gen/{name}.MODULE.bazel")'

    with deps_bzl.open("w") as f:
        f.write(deps_bzl_content)


def process_deps(deps, deps_bzl, is_shared=False):
    for repo in deps["repositories"]:
        gen_repo_bzl(repo)

    gen_deps_bzl(deps, deps_bzl, is_shared)


def strip_comments(text):
    # capture string literals first, comments send
    regex = re.compile(r"(\".*\")|(//.*$)", re.MULTILINE)
    return regex.sub(
        lambda match: "" if match.group(2) is not None else match.group(1), text
    )


def read_access_token():
    if not sys.stdin.isatty():
        return ""

    # 1. Try to obtain token from the gh tool
    try:
        res = subprocess.run(["gh", "auth", "token"], capture_output=True)
        if res.returncode == 0:
            return res.stdout.decode().strip()
        else:
            # User has gh but is not logged in
            print("Please log in to Github")
            print("$ gh auth login")
            raise SystemExit
    except FileNotFoundError:
        pass  # User does not have gh tool installed

        print(
            """Follow these steps to obtain a GitHub API access token with
appropriate permissions:

1. On github.com, go to
Settings > Developer Settings > Personal access tokens > Fine-grained tokens.
2. Generate a new token with default settings.

Alternatively, install the gh CLI tool to save time <https://github.com/cli/cli#installation>.
"""
        )
        print(
            "Please enter GitHub API access token (or empty to skip): ",
            end="",
            flush=True,
        )

        return sys.stdin.readline().strip("\n")


def process_config(deps_file):
    deps_path = SCRIPT_DIR / deps_file
    bzl_path = (GEN_DIR / deps_file).with_suffix(".MODULE.bazel")
    is_shared = deps_file == "shared_deps.jsonc"

    try:
        with deps_path.open() as fp:
            json_text = strip_comments(fp.read())
            process_deps(json.loads(json_text), bzl_path, is_shared)
    except FileNotFoundError:
        pass


def run():
    if TARGET_FILTER is None:
        global GITHUB_ACCESS_TOKEN
        GITHUB_ACCESS_TOKEN = read_access_token()

        for f in GEN_DIR.glob("*.bazel"):
            f.unlink()

    for deps in ALL_DEPS:
        process_config(deps)


run()
